{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping Tutorial\n",
    "\n",
    "This notebook demonstrates web scraping techniques using Python including:\n",
    "- Making HTTP requests with proper headers\n",
    "- Parsing HTML content with BeautifulSoup\n",
    "- Extracting specific content using CSS selectors\n",
    "- Understanding why User-Agent headers are important"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Environment\n",
    "\n",
    "Import the required libraries for web scraping: requests for HTTP requests and BeautifulSoup for HTML parsing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries for web scraping\n",
    "print(\"ğŸ“š Setting up the environment...\")\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "print(\"âœ… Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempt Without Headers\n",
    "\n",
    "First, try to access the webpage without proper headers to demonstrate why they're needed. Many websites block requests without User-Agent headers to prevent automated scraping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's try without headers to demonstrate why they're needed\n",
    "print(\"ğŸŒ Attempting to access webpage without headers...\")\n",
    "\n",
    "url = \"https://ceu.edu/article/2024-12-03/combining-data-science-society-thanika-haltrich-presidential-scholar-award\"\n",
    "\n",
    "try:\n",
    "    response = requests.get(url)\n",
    "    print(f\"ğŸ“¡ Response status code: {response.status_code}\")\n",
    "\n",
    "    if response.status_code == 403:\n",
    "        print(\"âŒ Access forbidden! This demonstrates why we need proper headers.\")\n",
    "        print(\"â„¹ï¸  Websites often block requests without proper User-Agent headers\")\n",
    "        print(\"   to prevent automated scraping.\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Request With Proper Headers\n",
    "\n",
    "Now make the request with a User-Agent header to successfully retrieve the webpage content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's try with proper headers\n",
    "print(\"ğŸŒ Attempting to access webpage with proper headers...\")\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "    + \" Chrome/39.0.2171.95 Safari/537.36\"\n",
    "}\n",
    "\n",
    "try:\n",
    "    response = requests.get(url, headers=headers)\n",
    "    print(f\"ğŸ“¡ Response status code: {response.status_code}\")\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Failed to fetch webpage: Status code {response.status_code}\")\n",
    "\n",
    "    content_length = len(response.content)\n",
    "    print(f\"âœ… Successfully retrieved the webpage! Received {content_length} bytes of data.\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse and Extract Content\n",
    "\n",
    "Parse the HTML using BeautifulSoup and extract the page title and article paragraphs using CSS selectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the HTML and extract content\n",
    "print(\"ğŸ” Parsing webpage content...\")\n",
    "\n",
    "try:\n",
    "    # Parse the HTML\n",
    "    webpage = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    # Extract title\n",
    "    title = webpage.title.string.strip()\n",
    "    print(\"\\nğŸ“‘ Page Title:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(title)\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    # Extract paragraphs\n",
    "    print(\"\\nğŸ“ Article Content:\")\n",
    "    print(\"-\" * 40)\n",
    "    description_html = webpage.select(\"#block-system-main p\")  # <----------------- !!! Our selector !!!\n",
    "    texts = [text.get_text().strip() for text in description_html]\n",
    "    text = \"\\n\".join(texts)\n",
    "    print(text)\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    print(\"\\nâœ… Content extracted successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error parsing content: {str(e)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
