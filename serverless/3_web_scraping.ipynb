{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping Tutorial\n",
    "\n",
    "This notebook demonstrates web scraping techniques using Python including:\n",
    "- Making HTTP requests with proper headers\n",
    "- Parsing HTML content with BeautifulSoup\n",
    "- Extracting specific content using CSS selectors\n",
    "- Understanding why User-Agent headers are important"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Environment\n",
    "\n",
    "Import the required libraries for web scraping: requests for HTTP requests and BeautifulSoup for HTML parsing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“š Setting up the environment...\n",
      "âœ… Libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries for web scraping\n",
    "print(\"ğŸ“š Setting up the environment...\")\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "print(\"âœ… Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempt Without Headers\n",
    "\n",
    "First, try to access the webpage without proper headers to demonstrate why they're needed. Many websites block requests without User-Agent headers to prevent automated scraping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸŒ Attempting to access webpage without headers...\n",
      "ğŸ“¡ Response status code: 403\n",
      "âŒ Access forbidden! This demonstrates why we need proper headers.\n",
      "â„¹ï¸  Websites often block requests without proper User-Agent headers\n",
      "   to prevent automated scraping.\n"
     ]
    }
   ],
   "source": [
    "# First, let's try without headers to demonstrate why they're needed\n",
    "print(\"ğŸŒ Attempting to access webpage without headers...\")\n",
    "\n",
    "url = \"https://www.ceu.edu/news/2025-11/cultivating-mindsets-and-practices-environmental-sustainability\"\n",
    "\n",
    "try:\n",
    "    response = requests.get(url)\n",
    "    print(f\"ğŸ“¡ Response status code: {response.status_code}\")\n",
    "\n",
    "    if response.status_code == 403:\n",
    "        print(\"âŒ Access forbidden! This demonstrates why we need proper headers.\")\n",
    "        print(\"â„¹ï¸  Websites often block requests without proper User-Agent headers\")\n",
    "        print(\"   to prevent automated scraping.\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Request With Proper Headers\n",
    "\n",
    "Now make the request with a User-Agent header to successfully retrieve the webpage content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸŒ Attempting to access webpage with proper headers...\n",
      "ğŸ“¡ Response status code: 200\n",
      "âœ… Successfully retrieved the webpage! Received 59136 bytes of data.\n"
     ]
    }
   ],
   "source": [
    "# Now let's try with proper headers\n",
    "print(\"ğŸŒ Attempting to access webpage with proper headers...\")\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "    + \" Chrome/39.0.2171.95 Safari/537.36\"\n",
    "}\n",
    "\n",
    "try:\n",
    "    response = requests.get(url, headers=headers)\n",
    "    print(f\"ğŸ“¡ Response status code: {response.status_code}\")\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Failed to fetch webpage: Status code {response.status_code}\")\n",
    "\n",
    "    content_length = len(response.content)\n",
    "    print(f\"âœ… Successfully retrieved the webpage! Received {content_length} bytes of data.\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install https://selectorgadget.com/ to extract different parts of the website."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse and Extract Content\n",
    "\n",
    "Parse the HTML using BeautifulSoup and extract the page title and article paragraphs using CSS selectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Parsing webpage content...\n",
      "\n",
      "ğŸ“‘ Page Title:\n",
      "----------------------------------------\n",
      "Cultivating Mindsets and Practices for Environmental Sustainability | Central European University\n",
      "----------------------------------------\n",
      "\n",
      "ğŸ“ Article Content:\n",
      "----------------------------------------\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "âœ… Content extracted successfully\n"
     ]
    }
   ],
   "source": [
    "# Parse the HTML and extract content\n",
    "print(\"ğŸ” Parsing webpage content...\")\n",
    "\n",
    "try:\n",
    "    # Parse the HTML\n",
    "    webpage = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    # Extract title\n",
    "    title = webpage.title.string.strip()\n",
    "    print(\"\\nğŸ“‘ Page Title:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(title)\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    # Extract paragraphs\n",
    "    print(\"\\nğŸ“ Article Content:\")\n",
    "    print(\"-\" * 40)\n",
    "    description_html = webpage.select(\".col-lg-8\")  # <----------------- !!! Our selector !!!\n",
    "    texts = [text.get_text().strip() for text in description_html]\n",
    "    text = \"\\n\".join(texts)\n",
    "    print(text)\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    print(\"\\nâœ… Content extracted successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error parsing content: {str(e)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
