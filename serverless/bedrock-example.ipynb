{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWS Bedrock Tutorial\n",
    "\n",
    "This notebook demonstrates how to use Amazon Bedrock for text generation including:\n",
    "- Assuming AWS IAM roles for secure access\n",
    "- Using the Titan text generation models\n",
    "- Understanding and calculating API costs\n",
    "- Configuring model parameters (temperature, max tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Configuration\n",
    "\n",
    "Import libraries, configure the model selection, and set up pricing information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This script demonstrates how to use Amazon Bedrock with role assumption\n",
    "# We'll use the Titan model for text generation and track the associated costs\n",
    "\n",
    "import json\n",
    "import os\n",
    "import pprint\n",
    "from typing import Dict\n",
    "\n",
    "import boto3\n",
    "\n",
    "USE_LITE = False\n",
    "# Pricing information for Titan models (as of March 2024)\n",
    "# Source: https://aws.amazon.com/bedrock/pricing/\n",
    "if USE_LITE:\n",
    "    MODEL_ID = \"amazon.titan-text-lite-v1\"  # We use Titan Lite for cost-effective text generation\n",
    "    COST_PER_INPUT_TOKEN = 0.0003 / 1000  # $0.0003 per 1,000 input tokens\n",
    "    COST_PER_OUTPUT_TOKEN = 0.0004 / 1000  # $0.0004 per 1,000 output tokens\n",
    "    print(f\"üöÄ Using Bedrock model {MODEL_ID}! This is a fast and cheap, but not super accurate model.\")\n",
    "else:\n",
    "    MODEL_ID = \"amazon.titan-text-express-v1\"  # We use Titan Express for more advanced text generation\n",
    "    COST_PER_INPUT_TOKEN = 0.001 / 1000  # $0.001 per 1,000 input tokens\n",
    "    COST_PER_OUTPUT_TOKEN = 0.0017 / 1000  # $0.0017 per 1,000 output tokens\n",
    "    print(f\"üöÄ Using Bedrock model {MODEL_ID}! This is a not so cheap, but quite accurate model.\")\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"üìö Setting up the environment...\")\n",
    "pp = pprint.PrettyPrinter(indent=2)\n",
    "# AWS Configuration\n",
    "# We need these constants to set up our AWS environment\n",
    "ROLE_ARN = \"arn:aws:iam::870137400553:role/BedrockUserRole\"  # The role we'll assume to access Bedrock\n",
    "REGION = \"us-east-1\"  # Bedrock is currently only available in specific regions\n",
    "\n",
    "print(\"‚úÖ Environment setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Role Assumption Function\n",
    "\n",
    "Create a function to assume an AWS IAM role and obtain temporary credentials for accessing Bedrock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assume_role(role_arn: str, session_name: str) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Assume an AWS IAM role to gain temporary security credentials.\n",
    "\n",
    "    This function helps us access AWS services (like Bedrock) using temporary credentials\n",
    "    obtained by assuming a role. It supports both AWS_PROFILE and default credentials.\n",
    "\n",
    "    Args:\n",
    "        role_arn (str): The Amazon Resource Name (ARN) of the role to assume\n",
    "        session_name (str): A name for the assumed role session\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, str]: Temporary credentials including AccessKeyId, SecretAccessKey, and SessionToken\n",
    "\n",
    "    Raises:\n",
    "        Exception: If role assumption fails due to permissions or network issues\n",
    "    \"\"\"\n",
    "    print(f\"üîê Attempting to assume role: {role_arn}\")\n",
    "\n",
    "    try:\n",
    "        # First, set up the initial AWS session\n",
    "        # We check if a specific AWS profile is requested through environment variables\n",
    "        if os.environ.get(\"AWS_PROFILE\"):\n",
    "            print(f\"Using AWS Profile: {os.environ['AWS_PROFILE']}\")\n",
    "            session = boto3.Session(profile_name=os.environ[\"AWS_PROFILE\"])\n",
    "        else:\n",
    "            print(\"Using default AWS credentials\")\n",
    "            session = boto3.Session()\n",
    "\n",
    "        # Use STS (Security Token Service) to assume the role\n",
    "        sts_client = session.client(\"sts\")\n",
    "        assumed_role = sts_client.assume_role(RoleArn=role_arn, RoleSessionName=session_name)\n",
    "        print(\"‚úÖ Role assumed successfully\")\n",
    "        return assumed_role[\"Credentials\"]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error assuming role: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "print(\"‚úÖ Function assume_role defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Cost Calculation Functions\n",
    "\n",
    "Create helper functions to estimate token counts and calculate API costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_token_count(text: str) -> int:\n",
    "    \"\"\"\n",
    "    Estimate the number of tokens in a text string.\n",
    "\n",
    "    This is a rough estimation - actual token count may vary.\n",
    "    We use a simple approximation of 4 characters per token.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to estimate tokens for\n",
    "\n",
    "    Returns:\n",
    "        int: Estimated number of tokens\n",
    "    \"\"\"\n",
    "    return len(text) // 4\n",
    "\n",
    "\n",
    "def calculate_cost(input_tokens: int, output_tokens: int) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the cost of a Bedrock request based on input and output tokens.\n",
    "\n",
    "    Args:\n",
    "        input_tokens (int): Number of input tokens\n",
    "        output_tokens (int): Number of output tokens\n",
    "\n",
    "    Returns:\n",
    "        float: Estimated cost in USD\n",
    "    \"\"\"\n",
    "    input_cost = input_tokens * COST_PER_INPUT_TOKEN\n",
    "    output_cost = output_tokens * COST_PER_OUTPUT_TOKEN\n",
    "    return input_cost + output_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Bedrock Message Function\n",
    "\n",
    "Create the main function to send text generation requests to Amazon Bedrock with cost tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_message_to_bedrock(message: str, max_tokens: int = 512, temperature: float = 0.7) -> str:\n",
    "    \"\"\"\n",
    "    Send a text generation request to Amazon Bedrock using the Titan model.\n",
    "\n",
    "    This function handles the entire process of:\n",
    "    1. Assuming the necessary AWS role\n",
    "    2. Setting up a Bedrock client\n",
    "    3. Sending the request\n",
    "    4. Processing the response\n",
    "    5. Calculating and displaying costs\n",
    "\n",
    "    Args:\n",
    "        message (str): The input text to send to the model\n",
    "        max_tokens (int, optional): Maximum number of tokens in the response. Defaults to 512.\n",
    "        temperature (float, optional): Controls randomness in the response.\n",
    "            0.0 is deterministic, 1.0 is most random. Defaults to 0.7.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated text response from the model\n",
    "\n",
    "    Raises:\n",
    "        Exception: If any step in the process fails\n",
    "    \"\"\"\n",
    "    print(\"üöÄ Preparing to send message to Bedrock...\")\n",
    "    print(f\"\\nüìù Input message: '{message}'\")\n",
    "\n",
    "    try:\n",
    "        # Step 1: Get temporary credentials through role assumption\n",
    "        credentials = assume_role(ROLE_ARN, \"BedrockSession\")\n",
    "\n",
    "        # Step 2: Create a new AWS session with our temporary credentials\n",
    "        session = boto3.Session(\n",
    "            aws_access_key_id=credentials[\"AccessKeyId\"],\n",
    "            aws_secret_access_key=credentials[\"SecretAccessKey\"],\n",
    "            aws_session_token=credentials[\"SessionToken\"],\n",
    "        )\n",
    "\n",
    "        # Step 3: Create Bedrock runtime client\n",
    "        bedrock_runtime = session.client(service_name=\"bedrock-runtime\", region_name=REGION)\n",
    "        # Step 4: Prepare the request payload for the AI model\n",
    "        payload = {\n",
    "            # The actual text prompt we want to send to the model\n",
    "            \"inputText\": message,\n",
    "            # Configuration settings that control how the model generates text\n",
    "            \"textGenerationConfig\": {\n",
    "                # Maximum number of tokens (word pieces) in the response\n",
    "                # Higher values allow longer responses but cost more\n",
    "                # 512 tokens is roughly 350-400 words\n",
    "                \"maxTokenCount\": max_tokens,\n",
    "                # List of sequences that will stop the generation when encountered\n",
    "                # Empty list means the model will continue until maxTokenCount\n",
    "                # Example: [\".\", \"?\", \"!\"] would stop at the first sentence end\n",
    "                \"stopSequences\": [],\n",
    "                # Controls randomness in the response (between 0.0 and 1.0)\n",
    "                # - Low values (0.0-0.3): More focused, deterministic responses\n",
    "                # - Medium values (0.4-0.7): Balanced creativity\n",
    "                # - High values (0.8-1.0): More random, creative responses\n",
    "                \"temperature\": temperature,\n",
    "                # Controls diversity of word choices (between 0.0 and 1.0)\n",
    "                # 1.0 means consider all options\n",
    "                # Lower values limit choices to only the most likely ones\n",
    "                # Most users should leave this at 1.0\n",
    "                \"topP\": 1,\n",
    "            },\n",
    "        }\n",
    "\n",
    "        print(\"\\nüì¶ Request configuration:\")\n",
    "        print(f\"- Model: {MODEL_ID}\")\n",
    "        print(f\"- Max tokens: {max_tokens}\")\n",
    "        print(f\"- Temperature: {temperature}\")\n",
    "\n",
    "        # Calculate and display estimated input tokens and cost\n",
    "        input_tokens = calculate_token_count(message)\n",
    "        print(\"\\nüí∞ Cost estimate (input):\")\n",
    "        print(f\"- Input tokens: ~{input_tokens}\")\n",
    "        print(f\"- Input cost: ${input_tokens * COST_PER_INPUT_TOKEN:.6f}\")\n",
    "\n",
    "        # Step 5: Send request to Bedrock\n",
    "        response = bedrock_runtime.invoke_model(\n",
    "            modelId=MODEL_ID,\n",
    "            contentType=\"application/json\",\n",
    "            accept=\"application/json\",\n",
    "            body=json.dumps(payload),\n",
    "        )\n",
    "\n",
    "        # Step 6: Process the response\n",
    "        response_body = json.loads(response[\"body\"].read())\n",
    "        output_text = response_body[\"results\"][0][\"outputText\"]\n",
    "\n",
    "        # Calculate and display estimated output tokens and total cost\n",
    "        output_tokens = calculate_token_count(output_text)\n",
    "        total_cost = calculate_cost(input_tokens, output_tokens)\n",
    "\n",
    "        print(\"\\nüí∞ Final cost calculation:\")\n",
    "        print(f\"- Output tokens: ~{output_tokens}\")\n",
    "        print(f\"- Output cost: ${output_tokens * COST_PER_OUTPUT_TOKEN:.6f}\")\n",
    "        print(f\"- Total cost: ${total_cost:.6f}\")\n",
    "\n",
    "        return output_text\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error sending message to Bedrock: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "print(\"‚úÖ Function send_message_to_bedrock defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Implementation\n",
    "\n",
    "Send a test message to Bedrock and display the AI-generated response with cost information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the implementation with different prompts\n",
    "# We'll try various types of requests to see how the model responds and track costs\n",
    "test_message = \"Write a haiku about data engineering\"\n",
    "\n",
    "try:\n",
    "    print(\"-\" * 40)\n",
    "    response = send_message_to_bedrock(test_message)\n",
    "    print(\"\\nü§ñ Titan's response:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(response.strip())\n",
    "    print(\"-\" * 40)\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Test failed: {str(e)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
