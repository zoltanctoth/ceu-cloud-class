{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikipedia Page Views Data Pipeline - Solutions\n",
    "\n",
    "This notebook demonstrates a simple ETL pipeline that:\n",
    "1. Extracts page view data from the Wikipedia REST API\n",
    "2. Transforms it to JSON Lines format\n",
    "3. Uploads directly to S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose Your Username\n",
    "\n",
    "Pick a unique username (e.g., your name or initials) and use it consistently:\n",
    "\n",
    "- **S3 Bucket**: `<username>-wikidata` (e.g., `johndoe-wikidata`)\n",
    "- **Athena Database**: `<username>` (e.g., `johndoe`)\n",
    "- **Lambda**: Use the same `<username>-wikidata` bucket\n",
    "\n",
    "**Important:** No hyphens in database names! Use underscores if needed (e.g., `john_doe`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Set your username here - use it consistently across all resources\nUSERNAME = \"zoltanctothceu\""
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import json\n",
    "\n",
    "import boto3\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract: Retrieve Data from Wikipedia Page Views API\n",
    "\n",
    "We use the Wikimedia Analytics API to fetch the most viewed pages for a specific date.\n",
    "\n",
    "**API Documentation:** https://doc.wikimedia.org/generated-data-platform/aqs/analytics-api/reference/page-views.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try different dates to see how the data changes\n",
    "DATE_PARAM = \"2025-11-17\"\n",
    "\n",
    "date = datetime.datetime.strptime(DATE_PARAM, \"%Y-%m-%d\")\n",
    "\n",
    "# Construct the API URL for top viewed pages\n",
    "url = f\"https://wikimedia.org/api/rest_v1/metrics/pageviews/top/en.wikipedia/all-access/{date.strftime('%Y/%m/%d')}\"\n",
    "print(f\"Requesting REST API URL: {url}\")\n",
    "\n",
    "# Make the API request\n",
    "wiki_server_response = requests.get(url, headers={\"User-Agent\": \"curl/7.68.0\"})\n",
    "wiki_response_status = wiki_server_response.status_code\n",
    "wiki_response_body = wiki_server_response.text\n",
    "\n",
    "print(f\"Wikipedia REST API Response body: {wiki_response_body[:500]}...\")\n",
    "print(f\"Wikipedia REST API Response Code: {wiki_response_status}\")\n",
    "\n",
    "# Validate response\n",
    "if wiki_response_status != 200:\n",
    "    raise Exception(f\"Received non-OK status code from Wiki Server: {wiki_response_status}\")\n",
    "print(f\"Successfully retrieved Wikipedia data, content-length: {len(wiki_response_body)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform: Process Raw Data into JSON Lines\n",
    "\n",
    "Convert the raw API response into a structured JSON Lines format suitable for analytics. Each line is a valid JSON object representing one page's view statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the API response and extract top views\n",
    "wiki_response_parsed = wiki_server_response.json()\n",
    "top_views = wiki_response_parsed[\"items\"][0][\"articles\"]\n",
    "\n",
    "# Transform to JSON Lines format\n",
    "current_time = datetime.datetime.now(datetime.timezone.utc)\n",
    "json_lines = \"\"\n",
    "for page in top_views:\n",
    "    record = {\n",
    "        \"title\": page[\"article\"],\n",
    "        \"views\": page[\"views\"],\n",
    "        \"rank\": page[\"rank\"],\n",
    "        \"date\": date.strftime(\"%Y-%m-%d\"),\n",
    "        \"retrieved_at\": current_time.replace(tzinfo=None).isoformat(),\n",
    "    }\n",
    "    json_lines += json.dumps(record) + \"\\n\"\n",
    "\n",
    "print(f\"Transformed {len(top_views)} records to JSON Lines\")\n",
    "print(f\"First few lines:\\n{json_lines[:500]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Upload to S3\n",
    "\n",
    "Upload the JSON Lines data directly to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S3_WIKI_BUCKET = f\"{USERNAME}-wikidata\"\n",
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "# Upload json_lines directly to S3\n",
    "s3_key = f\"raw-views/raw-views-{date.strftime('%Y-%m-%d')}.json\"\n",
    "s3.put_object(\n",
    "    Bucket=S3_WIKI_BUCKET,\n",
    "    Key=s3_key,\n",
    "    Body=json_lines,\n",
    ")\n",
    "print(f\"Uploaded {len(top_views)} records to s3://{S3_WIKI_BUCKET}/{s3_key}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify upload\n",
    "try:\n",
    "    s3.head_object(Bucket=S3_WIKI_BUCKET, Key=s3_key)\n",
    "    print(f\"File uploaded successfully to s3://{S3_WIKI_BUCKET}/{s3_key}\")\n",
    "except Exception as e:\n",
    "    print(f\"File not found at s3://{S3_WIKI_BUCKET}/{s3_key}\")\n",
    "    raise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}